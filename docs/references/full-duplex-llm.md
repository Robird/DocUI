# 全双工流式 LLM (Full-Duplex Streaming LLM)

> 状态: 完成
> 类型: 背景参考资料
> 关联 Key-Note: [llm-agent-context.md](../key-notes/llm-agent-context.md)

## 目的

澄清"流式 LLM"的两个不同含义，避免概念混淆。

---

## 1. 当前主流 LLM：半双工分块架构

```
┌─────────────────────────────────────────────────────────┐
│                    Half-Duplex LLM                       │
│                                                          │
│   Phase 1: Pre-Fill (批量输入)                           │
│   ┌─────────────────────────────────┐                   │
│   │ [System] [User] [History] ...   │ → KV Cache        │
│   └─────────────────────────────────┘                   │
│                                                          │
│   Phase 2: Decode (自回归输出)                           │
│   ┌─────────────────────────────────┐                   │
│   │ Token₁ → Token₂ → Token₃ → ... │ → Response        │
│   └─────────────────────────────────┘                   │
│                                                          │
│   ⚠️ 输入和输出是交替的，不能同时进行                     │
└─────────────────────────────────────────────────────────┘
```

**特点**：
- **批量 Pre-Fill**：一次性处理所有输入 token
- **批量 Decode**：逐个生成输出 token（自回归）
- **Half-Duplex**：输入完成后才能输出，输出时不能接收新输入

**SSE 流式传输**：
- 仅是**传输层优化**：Decode 阶段的 token 逐个发送到客户端
- 不改变模型架构：LLM 仍然是半双工的
- 类比：水龙头的水是"流式"出来的，但水管本身是单向的

---

## 2. 全双工流式 LLM：Think while Listening

```
┌─────────────────────────────────────────────────────────┐
│                   Full-Duplex LLM                        │
│                                                          │
│   Input Stream:   [I₁] [I₂] [I₃] [I₄] [I₅] ...         │
│                     ↓    ↓    ↓    ↓    ↓               │
│                   ┌─────────────────────┐               │
│                   │   Causal Attention  │               │
│                   │   with Dual Streams │               │
│                   └─────────────────────┘               │
│                     ↓    ↓    ↓    ↓    ↓               │
│   Output Stream:  [O₁] [∅]  [O₂] [∅]  [O₃] ...         │
│                                                          │
│   ✓ 输入和输出可以同时进行                               │
│   ✓ 支持抢答、插话、实时打断                            │
└─────────────────────────────────────────────────────────┘
```

**核心特性**：

- **最简形式**：每输入一个 token，模型被激活一次，可选输出一个 token
- **双流上下文**：外界输入流 + 自身输出流 共同构成模型上下文
- **空 Token (∅)**：特殊词表 token，表示"继续等待输入，暂不产生有效输出"
  - 与推理引擎有语义约定：连续空 token 不放入上下文 buffer
  - 类比：Stop Token、OpenAI Responses API 的 Channel Token 也是特殊控制 token

**场景示例 — 小学门卫 Agent**：

> 你是小学大门的监控与考勤 Agent。一个迟到的小朋友对着摄像头和麦克风滔滔不绝地讲他半真半假的迟到理由，试图说服你开门且不计入考勤。
>
> 在此场景中：
> - 收到 token "猫落水" 时，你可能产生心理活动：*"他说猫落水了，可他的头发和衣服看起来很干燥..."*
> - 你可能**打断**他的话
> - 你也可能需要**继续听更多**——这时输出空 token ∅，不增长输出 buffer

**技术实现**：

核心洞察：**不需要对现有 Transformer 技术进行大改**。

1. **位置编码区间划分**：
   - 给输入流和输出流分配不同的位置编码空间
   - 例如：输入用 [0, N)，输出用 [N, 2N)
   - 使得交错的 token 在逻辑上被理解为两个各自稠密的流

2. **Channel 编码**（替代方案）：
   - 给每个 token 额外加一个"来源标识" embedding
   - 优势：支持多输入多输出通道（控制/数据信道、多模态视频/音频流）
   - 劣势：不如位置编码技术成熟

3. **因果注意力 Mask**：
   - **与现有 LLM 完全相同**
   - 所有 token 按产生顺序记录在 buffer 中
   - Self-attention 和 mask 依然按 buffer 中的先后顺序工作
   - 输出流不能看到"未来"的输入，严格保持时序因果

**训练数据格式**：

两个交错的序列，各自进行位置编码。示意：

```
Buffer 内容（u=用户输入，m=模型输出）：

u这次 m∅ u真的 m∅ u是 m∅ u我的 m∅ u猫 u掉水里 u了 u， m放屁！ u我 m你 u要是 m身上 u不去救 m没有 u它就 m一点 u淹死了 m水...

位置编码：
- 用户流 [0, N): 这次, 真的, 是, 我的, 猫, 掉水里, 了, ..., 我, 要是, 不去救, 它就, 淹死了
- 模型流 [N, 2N): ∅, ∅, ∅, ∅, 放屁！, 你, 身上, 没有, 一点, 水...
```

**应用场景**：

- 语音交互：支持抢答与插话
- 实时翻译：边听边译
- 人机协作：实时反馈与修正
- 低延迟信号监视：最低 1 个 token 的处理延迟

---

## 3. 控制/数据信道分离：Agent 协作的新范式

全双工 LLM 的一个重要能力是**控制信道与数据信道的分离**。

**场景：编写代码时的即时工具调用**

> 设想你正在写代码，输出了 `Session.` 之后，意识到记不清关闭 Session 的方法是叫 `Close` 还是 `Stop`。
>
> **当前半双工 LLM 的做法**：
> - 硬着头皮写完后续代码
> - 等待编译器报错后调整
> - 或者在 Tool-Call 时打断自己的输出
>
> **全双工 LLM + 控制信道的做法**：
> - 暂停数据 token 的输出
> - 通过控制信道发起 Tool-Call（让 Roslyn 查阅成员列表）
> - 结果返回到上下文
> - 有信心地继续编写后续代码

```
数据信道:  Session.  [暂停] ─────────────────────── Close();
                        │                              ↑
控制信道:               └─ [tool:list_members Session] ┘
                              │
                              ↓
                        {Close(), Dispose(), ...}
```

**低延迟信号监视**：

全双工流式工作模式使 LLM 能胜任需要低延迟的信号监视场景：
- 最低能以 **1 个 token 的处理延迟** 对目标信号做出反应
- 适用于实时告警、交易信号、安全监控等场景

**迁移成本预估**：

基于对不连续 KV Cache 实验的了解，现有已训练好的开源 LLM **可能只需要少量微调**就能工作在双输入流上。核心原因：
- Attention 机制本身不变
- 只是位置编码的解释方式改变
- 模型已经学会了处理长序列中的远距离依赖

---

## 4. 对比总结

| 维度 | 半双工 LLM (当前主流) | 全双工 LLM |
|------|----------------------|------------|
| **通讯模式** | Half-Duplex | Full-Duplex |
| **输入输出** | 交替进行 | 可同时进行 |
| **响应延迟** | 必须等输入完成 | 可实时响应 |
| **打断能力** | 不支持 | 支持 |
| **SSE 流式** | 传输层优化 | 架构层特性 |
| **典型应用** | Chat、Coding Agent | 语音助手、实时翻译 |

---

## 5. 对 DocUI 的影响

**当前设计**：DocUI 基于半双工分块通讯模型，对应主流 LLM 能力。

**未来扩展**：若需支持全双工 LLM，可能需要：
- 增量 Observation 更新机制（流式推送状态变化）
- 实时锚点状态同步（Button/Form 的即时反馈）
- 中断/恢复协议（处理抢答和插话）

---

## 参考资料

本文档基于未公开的研究方向整理，暂无公开论文。

核心思想：**全双工 LLM 是现有 Transformer 架构的自然扩展**，主要通过位置编码区间划分实现，不需要对 attention 机制进行大改。
